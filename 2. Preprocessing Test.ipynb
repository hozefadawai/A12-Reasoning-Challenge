{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBP1XaINGObI"
      },
      "source": [
        "## PREPROCESSING TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpeSAzmGvaNx",
        "outputId": "09f2e5d7-a785-483a-8e89-b9569a84e119"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import operator\n",
        "from collections import defaultdict\n",
        "import ast\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFfPNltPzZ1i"
      },
      "outputs": [],
      "source": [
        "def clean_text(corpus):\n",
        "\n",
        "    #replacing whatever is in two backward slash with space\n",
        "    corpus = re.sub(r\"\\\\.*?\\\\\", ' ',corpus)\n",
        "    # replacing all the words ending with ':'\n",
        "    corpus = re.sub('\\S.*(?<=:)', ' ',corpus)\n",
        "    # replacing whatever is in angular brackets with space\n",
        "    corpus = re.sub(r\"\\<.*\\>\", ' ',corpus)\n",
        "    # replacing whatever is in paranthesis with space\n",
        "    corpus = re.sub(r\"\\(.*\\)\", ' ',corpus)\n",
        "    #replacing whatever is in square brackets with space\n",
        "    corpus = re.sub(r\"\\[.*\\]\", ' ',corpus)\n",
        "    #replacing whatever is in double square brackets with space\n",
        "    corpus = re.sub(r\"\\[\\[.*?\\]\\]\", ' ',corpus)\n",
        "    #replacing whatever is in braces with space\n",
        "    corpus = re.sub(r\"\\{.*\\}\", ' ',corpus)\n",
        "    #replacing whatever is in double braces with space\n",
        "    corpus = re.sub(r\"\\{{.*\\}}\", ' ',corpus)\n",
        "    #replacing whatever is in hyphen with space\n",
        "    corpus = re.sub(r'-.*?-', ' ',corpus)\n",
        "    #replacing new line like \\n and \\t with space\n",
        "    corpus = re.sub(r'\\n|\\t', ' ',corpus)\n",
        "    #replacing single charater ending with fullstop like 'a.' or multiple charater aswell 'a.b.c.' with space\n",
        "    corpus = re.sub(r'\\b\\w\\.(?!\\w)|\\b\\w(\\.\\w)+\\b', ' ',corpus)\n",
        "\n",
        "\n",
        "    # url starting with https:// or http:// or www.\n",
        "    corpus = re.sub('http://\\S+|https://\\S+|www.\\S+', '', corpus)\n",
        "\n",
        "    # removing trailing and leading underscore from the words\n",
        "    x = [i.strip('_') for i in corpus.split()]\n",
        "    corpus = ' '.join(x)\n",
        "\n",
        "    # removing two and one letter words before underscore\n",
        "    x=[]\n",
        "    # splitting words\n",
        "    for i in corpus.split():\n",
        "        # if underscore _ is there then\n",
        "        if '_' in i:\n",
        "            # it will take word before underscore\n",
        "            # and check if it is greater two\n",
        "            if len(i.split('_')[0]) > 2:\n",
        "                # if it is greater than 2 then it will append\n",
        "                x.append(i)\n",
        "            # if the word before underscore is less than or equal to 2\n",
        "            if len(i.split('_')[0]) <= 2:\n",
        "                # then it will only append word after the underscore\n",
        "                x.append(i.split('_')[1])\n",
        "        else:\n",
        "            x.append(i)\n",
        "    corpus= ' '.join(x)\n",
        "\n",
        "\n",
        "\n",
        "    # Decontractions\n",
        "    corpus = re.sub(r\"he's\", \"he is\", corpus)\n",
        "    corpus = re.sub(r\"there's\", \"there is\", corpus)\n",
        "    corpus = re.sub(r\"We're\", \"We are\", corpus)\n",
        "    corpus = re.sub(r\"That's\", \"That is\", corpus)\n",
        "    corpus = re.sub(r\"won't\", \"will not\", corpus)\n",
        "    corpus = re.sub(r\"they're\", \"they are\", corpus)\n",
        "    corpus = re.sub(r\"Can't\", \"Cannot\", corpus)\n",
        "    corpus = re.sub(r\"wasn't\", \"was not\", corpus)\n",
        "    corpus = re.sub(r\"dont\", \"do not\", corpus)\n",
        "    corpus = re.sub(r\"aren't\", \"are not\", corpus)\n",
        "    corpus = re.sub(r\"isn't\", \"is not\", corpus)\n",
        "    corpus = re.sub(r\"What's\", \"What is\", corpus)\n",
        "    corpus = re.sub(r\"haven't\", \"have not\", corpus)\n",
        "    corpus = re.sub(r\"hasn't\", \"has not\", corpus)\n",
        "    corpus = re.sub(r\"There's\", \"There is\", corpus)\n",
        "    corpus = re.sub(r\"He's\", \"He is\", corpus)\n",
        "    corpus = re.sub(r\"It's\", \"It is\", corpus)\n",
        "    corpus = re.sub(r\"You're\", \"You are\", corpus)\n",
        "    corpus = re.sub(r\"I'M\", \"I am\", corpus)\n",
        "    corpus = re.sub(r\"shouldn't\", \"should not\", corpus)\n",
        "    corpus = re.sub(r\"wouldn't\", \"would not\", corpus)\n",
        "    corpus = re.sub(r\"i'm\", \"I am\", corpus)\n",
        "    corpus = re.sub(r\"Im\", \"I am\", corpus)\n",
        "    corpus = re.sub(r\"I'm\", \"I am\", corpus)\n",
        "    corpus = re.sub(r\"Isn't\", \"is not\", corpus)\n",
        "    corpus = re.sub(r\"Here's\", \"Here is\", corpus)\n",
        "    corpus = re.sub(r\"you've\", \"you have\", corpus)\n",
        "    corpus = re.sub(r\"youve\", \"you have\", corpus)\n",
        "    corpus = re.sub(r\"we're\", \"we are\", corpus)\n",
        "    corpus = re.sub(r\"what's\", \"what is\", corpus)\n",
        "    corpus = re.sub(r\"couldn't\", \"could not\", corpus)\n",
        "    corpus = re.sub(r\"we've\", \"we have\", corpus)\n",
        "    corpus = re.sub(r\"its\", \"it is\", corpus)\n",
        "    corpus = re.sub(r\"doesnt\", \"does not\", corpus)\n",
        "    corpus = re.sub(r\"Its\", \"It is\", corpus)\n",
        "    corpus = re.sub(r\"Heres\", \"Here is\", corpus)\n",
        "    corpus = re.sub(r\"who's\", \"who is\", corpus)\n",
        "    corpus = re.sub(r\"Ive\", \"I have\", corpus)\n",
        "    corpus = re.sub(r\"y'all\", \"you all\", corpus)\n",
        "    corpus = re.sub(r\"cant\", \"cannot\", corpus)\n",
        "    corpus = re.sub(r\"would've\", \"would have\", corpus)\n",
        "    corpus = re.sub(r\"it'll\", \"it will\", corpus)\n",
        "    corpus = re.sub(r\"we'll\", \"we will\", corpus)\n",
        "    corpus = re.sub(r\"wouldnt\", \"would not\", corpus)\n",
        "    corpus = re.sub(r\"We've\", \"We have\", corpus)\n",
        "    corpus = re.sub(r\"he'll\", \"he will\", corpus)\n",
        "    corpus = re.sub(r\"Y'all\", \"You all\", corpus)\n",
        "    corpus = re.sub(r\"Weren't\", \"Were not\", corpus)\n",
        "    corpus = re.sub(r\"Didn't\", \"Did not\", corpus)\n",
        "    corpus = re.sub(r\"they'll\", \"they will\", corpus)\n",
        "    corpus = re.sub(r\"they'd\", \"they would\", corpus)\n",
        "    corpus = re.sub(r\"DON'T\", \"DO NOT\", corpus)\n",
        "    corpus = re.sub(r\"Thats\", \"That is\", corpus)\n",
        "    corpus = re.sub(r\"they've\", \"they have\", corpus)\n",
        "    corpus = re.sub(r\"i'd\", \"I would\", corpus)\n",
        "    corpus = re.sub(r\"should've\", \"should have\", corpus)\n",
        "    corpus = re.sub(r\"Youre\", \"You are\", corpus)\n",
        "    corpus = re.sub(r\"where's\", \"where is\", corpus)\n",
        "    corpus = re.sub(r\"Dont\", \"Do not\", corpus)\n",
        "    corpus = re.sub(r\"we'd\", \"we would\", corpus)\n",
        "    corpus = re.sub(r\"i'll\", \"I will\", corpus)\n",
        "    corpus = re.sub(r\"weren't\", \"were not\", corpus)\n",
        "    corpus = re.sub(r\"They're\", \"They are\", corpus)\n",
        "    corpus = re.sub(r\"Cant\", \"Cannot\", corpus)\n",
        "    corpus = re.sub(r\"youll\", \"you will\", corpus)\n",
        "    corpus = re.sub(r\"Id\", \"I would\", corpus)\n",
        "    corpus = re.sub(r\"let's\", \"let us\", corpus)\n",
        "    corpus = re.sub(r\"it's\", \"it is\", corpus)\n",
        "    corpus = re.sub(r\"can't\", \"cannot\", corpus)\n",
        "    corpus = re.sub(r\"don't\", \"do not\", corpus)\n",
        "    corpus = re.sub(r\"you're\", \"you are\", corpus)\n",
        "    corpus = re.sub(r\"i've\", \"I have\", corpus)\n",
        "    corpus = re.sub(r\"that's\", \"that is\", corpus)\n",
        "    corpus = re.sub(r\"i'll\", \"I will\", corpus)\n",
        "    corpus = re.sub(r\"doesn't\", \"does not\", corpus)\n",
        "    corpus = re.sub(r\"i'd\", \"I would\", corpus)\n",
        "    corpus = re.sub(r\"didn't\", \"did not\", corpus)\n",
        "    corpus = re.sub(r\"ain't\", \"am not\", corpus)\n",
        "    corpus = re.sub(r\"you'll\", \"you will\", corpus)\n",
        "    corpus = re.sub(r\"I've\", \"I have\", corpus)\n",
        "    corpus = re.sub(r\"Don't\", \"do not\", corpus)\n",
        "    corpus = re.sub(r\"I'll\", \"I will\", corpus)\n",
        "    corpus = re.sub(r\"I'd\", \"I would\", corpus)\n",
        "    corpus = re.sub(r\"Let's\", \"Let us\", corpus)\n",
        "    corpus = re.sub(r\"you'd\", \"You would\", corpus)\n",
        "    corpus = re.sub(r\"It's\", \"It is\", corpus)\n",
        "    corpus = re.sub(r\"Ain't\", \"am not\", corpus)\n",
        "    corpus = re.sub(r\"Haven't\", \"Have not\", corpus)\n",
        "    corpus = re.sub(r\"Could've\", \"Could have\", corpus)\n",
        "    corpus = re.sub(r\"youve\", \"you have\", corpus)\n",
        "    corpus = re.sub(r\"dont\", \"do not\", corpus)\n",
        "    corpus = re.sub(r\"wasnt\", \"was not\", corpus)\n",
        "\n",
        "\n",
        "    #lowering all the words\n",
        "    corpus = corpus.lower()\n",
        "\n",
        "    # replacing all the digits with space\n",
        "    corpus = re.sub('[0-9]', '', corpus)\n",
        "\n",
        "    # Text chunking\n",
        "    words = word_tokenize(corpus)\n",
        "    tagged = pos_tag(words)\n",
        "    chunks = ne_chunk(tagged)\n",
        "    person = []\n",
        "    place= []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            if chunk.label() == 'GPE':\n",
        "                if type(chunk) is nltk.Tree:\n",
        "                    t= ' '.join(c[0] for c in chunk.leaves())\n",
        "                    place.append(t)\n",
        "\n",
        "            if chunk.label() == 'PERSON':\n",
        "                if type(chunk) is nltk.Tree:\n",
        "                    t= ' '.join(c[0] for c in chunk.leaves())\n",
        "                    person.append(t)\n",
        "\n",
        "    # replacing space between two letter place words with underscore '_'\n",
        "    place = re.sub(r'[^a-z]+', ' ', str(place))\n",
        "    place_= list(map(lambda x: x.replace(' ','_'), place))\n",
        "    place_ = re.sub(r'[^a-z]+', ' ', str(place_))\n",
        "\n",
        "    # now replacing place words with words with undercore in between\n",
        "    for k, j in enumerate(place):\n",
        "        corpus = re.sub(str(j), place_[k], corpus)\n",
        "    # replacing person name with space\n",
        "    corpus = ' '.join(e for e in corpus.split() if e not in person)\n",
        "\n",
        "    # removing trailing and leading underscore from the words\n",
        "    x = [i.strip('_') for i in corpus.split()]\n",
        "    corpus = ' '.join(x)\n",
        "\n",
        "    # keeping words which are greater than 3\n",
        "    corpus = re.findall(r'\\b\\w{3,}\\b', corpus)\n",
        "    corpus= ' '.join(corpus)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # replacing all the words except \"A-Za-z_\" with space\n",
        "    corpus = re.sub(r'[^a-z_]+', ' ', corpus)\n",
        "    preprocessed_text = (''.join(corpus))\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG-IwND3hl9L"
      },
      "outputs": [],
      "source": [
        "def data_cleaning(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    corpus= []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # open text corpus as ascii to avoid all the Unicode characters\n",
        "    with open(text, 'r',  encoding= 'ascii', errors='ignore') as f:\n",
        "\n",
        "        # for every line the corpus\n",
        "        for line in f:\n",
        "           count+= 1\n",
        "           # it will clean the text line by line\n",
        "           line = clean_text(line)\n",
        "           # if the text is empty it will continue the loop\n",
        "           if line == ' ': continue\n",
        "\n",
        "           #tokenizing\n",
        "           line= word_tokenize(line)\n",
        "           # removing stopwords\n",
        "           line = [word for word in line if word not in stop_words]\n",
        "           line = ' '.join(line)\n",
        "           corpus.append(line)\n",
        "\n",
        "    f.close\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdxYGZD5j8p2"
      },
      "outputs": [],
      "source": [
        "corpus_arc = data_cleaning('/content/ARC-V1-Feb2018-2/ARC_Corpus.txt')\n",
        "corpus_aristo = data_cleaning('/content/Aristo-Mini-Corpus-Dec2016/Aristo-Mini-Corpus-Dec2016.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTDtQYn_gU7G"
      },
      "outputs": [],
      "source": [
        "pickle.dump((corpus_arc, corpus_aristo),\n",
        "            open('/content/drive/MyDrive/my assignments/33. A12 Reasoning Challenge- Self case study 2/corpus_arc_aristo.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MAv9D6JgIyQ"
      },
      "outputs": [],
      "source": [
        "corpus_arc, corpus_aristo= \\\n",
        "pickle.load(open('/content/drive/MyDrive/my assignments/33. A12 Reasoning Challenge- Self case study 2/corpus_arc_aristo.pkl', 'rb'))"
      ]
    }
  ]
}