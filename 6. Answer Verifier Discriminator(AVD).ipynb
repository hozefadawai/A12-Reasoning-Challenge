{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk-fE62i-uGg"
      },
      "source": [
        "# Answer Verifier Discriminator(AVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dh__0Rneeg5",
        "outputId": "1dfea03c-e5e6-4f11-9beb-a7d05525c143"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import operator\n",
        "from collections import defaultdict\n",
        "import ast\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KtmuZspE6Ld"
      },
      "source": [
        "###Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKDZNSgkELO",
        "outputId": "b2e2153f-7664-4eb7-984a-2e90e60a98a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-10 17:29:16--  http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25443609 (24M) [application/x-gzip]\n",
            "Saving to: ‘RACE.tar.gz’\n",
            "\n",
            "RACE.tar.gz         100%[===================>]  24.26M  3.18MB/s    in 8.2s    \n",
            "\n",
            "2023-11-10 17:29:25 (2.96 MB/s) - ‘RACE.tar.gz’ saved [25443609/25443609]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading RACE Dataset\n",
        "!wget http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn7I67QFk0tW"
      },
      "outputs": [],
      "source": [
        "# extracting dataset\n",
        "import tarfile\n",
        "with tarfile.open('/content/RACE.tar.gz') as tar:\n",
        "\n",
        "    tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs6wAi0ymo2Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def load_race(location):\n",
        "\n",
        "    file_ = location\n",
        "    race = pd.DataFrame()\n",
        "\n",
        "    for txt in os.listdir(file_):\n",
        "        with open(location + txt, 'r') as f:\n",
        "            race= pd.concat([race, pd.read_json(f.read())], ignore_index=True)\n",
        "\n",
        "    return race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78W1h5XolI2H"
      },
      "outputs": [],
      "source": [
        "race_train_high= load_race('/content/RACE/train/high/')\n",
        "race_dev_high= load_race('/content/RACE/dev/high/')\n",
        "race_test_high= load_race('/content/RACE/test/high/')\n",
        "\n",
        "race_train_medium= load_race('/content/RACE/train/middle/')\n",
        "race_dev_medium= load_race('/content/RACE/dev/middle/')\n",
        "race_test_medium= load_race('/content/RACE/test/middle/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT1E4HhTGNXb"
      },
      "outputs": [],
      "source": [
        "def data_cleaning(context):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    corpus= []\n",
        "\n",
        "        # for every line the corpus\n",
        "    for i in context:\n",
        "        # it will clean the text line by line\n",
        "        line = clean_text(i)\n",
        "        # if the text is empty it will continue the loop\n",
        "        if line == ' ': continue\n",
        "        #tokenizing\n",
        "        line= word_tokenize(line)\n",
        "        # removing stopwords\n",
        "        line = [word for word in line if word not in stop_words]\n",
        "        line = ' '.join(line)\n",
        "        corpus.append(line)\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dnM7gpJFV5P"
      },
      "outputs": [],
      "source": [
        "# cleaning and preprocessing context data\n",
        "\n",
        "race_train_high['cleaned_article'] = data_cleaning(race_train_high['article'])\n",
        "race_dev_high['cleaned_article'] = data_cleaning(race_dev_high['article'])\n",
        "race_test_high['cleaned_article'] = data_cleaning(race_test_high['article'])\n",
        "\n",
        "race_train_medium['cleaned_article'] = data_cleaning(race_train_medium['article'])\n",
        "race_dev_medium['cleaned_article'] = data_cleaning(race_dev_medium['article'])\n",
        "race_test_medium['cleaned_article'] = data_cleaning(race_test_medium['article'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8p0jSgrfKTt"
      },
      "outputs": [],
      "source": [
        "pickle.dump((race_train_high, race_dev_high, race_test_high, race_train_medium, race_dev_medium, race_test_medium),\n",
        "            open('/content/drive/MyDrive/my assignments/33. A12 Reasoning Challenge- Self case study 2/race_datasets.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSjZ6oOfKTu"
      },
      "outputs": [],
      "source": [
        "race_train_high, race_dev_high, race_test_high, race_train_medium, race_dev_medium, race_test_medium= \\\n",
        "pickle.load(open('/content/drive/MyDrive/my assignments/33. A12 Reasoning Challenge- Self case study 2/race_datasets.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "cJAyOw7xMHNl",
        "outputId": "321075ba-bb37-4d76-e39c-b80ffadfb5c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7c7df52a-63e5-464a-a15f-eaab36ac7e62\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answers</th>\n",
              "      <th>options</th>\n",
              "      <th>questions</th>\n",
              "      <th>article</th>\n",
              "      <th>id</th>\n",
              "      <th>cleaned_article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>[there are few doctors there, the nearest doctor is sometimes very far away from them, there is always heavy traffic on the road, they don't want to see a doctor]</td>\n",
              "      <td>Some people in the Australian outback can't get to a doctor quickly. Because  _</td>\n",
              "      <td>Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"</td>\n",
              "      <td>high9717.txt</td>\n",
              "      <td>people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B</td>\n",
              "      <td>[by sea, by air, on foot, in a car]</td>\n",
              "      <td>The doctor there usually goes to visit his patient  _  .</td>\n",
              "      <td>Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"</td>\n",
              "      <td>high9717.txt</td>\n",
              "      <td>people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>[tells him to have a rest, tells him what food to eat, gives him some advice, gives him some medicine]</td>\n",
              "      <td>If the person isn't very sick, the doctor only   _  .</td>\n",
              "      <td>Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"</td>\n",
              "      <td>high9717.txt</td>\n",
              "      <td>people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c7df52a-63e5-464a-a15f-eaab36ac7e62')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c7df52a-63e5-464a-a15f-eaab36ac7e62 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c7df52a-63e5-464a-a15f-eaab36ac7e62');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ca6bf4ca-63ef-45c9-9944-11e955cf8edd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca6bf4ca-63ef-45c9-9944-11e955cf8edd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ca6bf4ca-63ef-45c9-9944-11e955cf8edd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  answers  \\\n",
              "0       B   \n",
              "1       B   \n",
              "2       C   \n",
              "\n",
              "                                                                                                                                                              options  \\\n",
              "0  [there are few doctors there, the nearest doctor is sometimes very far away from them, there is always heavy traffic on the road, they don't want to see a doctor]   \n",
              "1                                                                                                                                 [by sea, by air, on foot, in a car]   \n",
              "2                                                              [tells him to have a rest, tells him what food to eat, gives him some advice, gives him some medicine]   \n",
              "\n",
              "                                                                         questions  \\\n",
              "0  Some people in the Australian outback can't get to a doctor quickly. Because  _   \n",
              "1                         The doctor there usually goes to visit his patient  _  .   \n",
              "2                            If the person isn't very sick, the doctor only   _  .   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  article  \\\n",
              "0  Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"   \n",
              "1  Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"   \n",
              "2  Most people go to a doctor in their own town or suburb  .  But people in the Australian _ can't get to a doctor quickly. The nearest doctor is sometimes hundreds of kilometers away so they have to call him on a two-way radio. This special doctor is called the \"flying doctor\". He visits sick people by plane.\\nWhen someone is very sick, the doctor has to fly to the person's home. His plane lands on a flat piece of ground near the person's house. Sometimes the doctor has to take the patient to hospital. Flying doctors take about 8,600 people to hospital each year.\\nHowever, most of the time the person isn't very sick, and the doctor doesn't have to visit. He can give advice on the radio from the office at the flying doctor center. He can tell the patient to use some medicine from a special medicine chest  . There is one of these chests in every home in the outback. Each bottle, tube   and packet   in the chest has a number. The doctor often says something like this,\" take two tablets   from bottle 5 every four hours.\"   \n",
              "\n",
              "             id  \\\n",
              "0  high9717.txt   \n",
              "1  high9717.txt   \n",
              "2  high9717.txt   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    cleaned_article  \n",
              "0  people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours  \n",
              "1  people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours  \n",
              "2  people doctor town suburb people australian get doctor quickly nearest doctor sometimes hundreds kilometers away call two way radio special doctor called flying doctor visit sick people plane someone sick doctor fly person home plane lands flat piece ground near person house sometimes doctor take patient hospital flying doctors take people hospital year however time person sick doctor visit give advice radio office flying doctor center tell patient use medicine special medicine chest one chests every home outback bottle tube packet chest number doctor often says something like take two tablets bottle every four hours  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "race_train_high.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-VsVxvqFBb5"
      },
      "source": [
        "### Labels Transformation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xONP-leQYw2u"
      },
      "outputs": [],
      "source": [
        "def get_labels(dataset):\n",
        "# Initialize empty lists for DataFrame columns\n",
        "    questions = []\n",
        "    choices = []\n",
        "    contexts = []\n",
        "    labels = []\n",
        "\n",
        "    ans={'A':1,\n",
        "         'B':2,\n",
        "         'C':3,\n",
        "         'D':4}\n",
        "\n",
        "    # Process each example in the RACE dataset\n",
        "    for index, row in dataset.iterrows():\n",
        "        # Create rows for each choice\n",
        "        questions.append(row[\"questions\"])\n",
        "        choices.append(row[\"options\"])\n",
        "        contexts.append(row[\"cleaned_article\"])\n",
        "        labels.append(ans[row['answers']])\n",
        "\n",
        "    # Create a DataFrame from the lists\n",
        "    df = pd.DataFrame({\n",
        "        \"Question\": questions,\n",
        "        \"Choice\": choices,\n",
        "        \"Context\": contexts,\n",
        "        \"Label\": labels\n",
        "    })\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsWdedfD2w_f"
      },
      "outputs": [],
      "source": [
        "# it will transform alphabetical labels to numerical formal\n",
        "new_race_train_high=  get_labels(race_train_high)\n",
        "new_race_dev_high= get_labels(race_dev_high)\n",
        "new_race_test_high= get_labels(race_test_high)\n",
        "\n",
        "new_race_train_medium= get_labels(race_train_medium)\n",
        "new_race_dev_medium= get_labels(race_dev_medium)\n",
        "new_race_test_medium= get_labels(race_test_medium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkM871RFSLt"
      },
      "source": [
        "## Getting Model Ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0GXuX-Si875"
      },
      "outputs": [],
      "source": [
        "# here we are downloading preprocessed saved RACE dataset from Gdrive\n",
        "!gdown 1SrFfAO_RfjC7ckNhJDCJzhWwNekBOd0i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09fU1h8raz5Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "# import matplotlib as plt\n",
        "# import seaborn as sns\n",
        "# import re\n",
        "# import string\n",
        "# import operator\n",
        "# from collections import defaultdict\n",
        "# import ast\n",
        "import pickle\n",
        "\n",
        "# # import nltk\n",
        "# from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "# # from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm_I7fqejBmC"
      },
      "outputs": [],
      "source": [
        "# pickle load data\n",
        "race_train_high, race_dev_high, race_test_high, race_train_medium, race_dev_medium, race_test_medium= \\\n",
        "pickle.load(open('/content/race_datasets.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5OmHXdzDBLP",
        "outputId": "e3033a61-52d3-4307-fd8d-df6fa1c88e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 xxhash-3.5.0 yarl-1.9.4\n",
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.10/dist-packages (0.10)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.63.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (5.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2024.7.4)\n",
            "Collecting accelerate==0.26.1\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (2.3.0+cpu)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.1) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.1) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
            "Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.26.1\n",
            "Looking in links: https://storage.googleapis.com/libtpu-releases/index.html\n",
            "Requirement already satisfied: torch~=2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0+cpu)\n",
            "Requirement already satisfied: torch_xla~=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch_xla[tpu]~=2.3.0) (2.3.0+libtpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.3.0) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (1.4.0)\n",
            "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (0.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (6.0.2)\n",
            "Collecting libtpu-nightly==0.1.dev20240322 (from torch_xla[tpu]~=2.3.0)\n",
            "  Downloading https://storage.googleapis.com/libtpu-nightly-releases/wheels/libtpu-nightly/libtpu_nightly-0.1.dev20240322%2Bdefault-py3-none-any.whl (115.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.3.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (1.63.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (5.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.3.0->torch_xla[tpu]~=2.3.0) (2024.7.4)\n",
            "Installing collected packages: libtpu-nightly\n",
            "  Attempting uninstall: libtpu-nightly\n",
            "    Found existing installation: libtpu-nightly 0.1.dev20240403+default\n",
            "    Uninstalling libtpu-nightly-0.1.dev20240403+default:\n",
            "      Successfully uninstalled libtpu-nightly-0.1.dev20240403+default\n",
            "Successfully installed libtpu-nightly-0.1.dev20240322+default\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers evaluate datasets sentencepiece\n",
        "! pip install cloud-tpu-client==0.10\n",
        "# ! pip install https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
        "! pip install accelerate==0.26.1\n",
        "! pip install torch~=2.3.0 torch_xla[tpu]~=2.3.0 -f https://storage.googleapis.com/libtpu-releases/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xoUmV_4MhUZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ0ce5DKuFzA",
        "outputId": "7a71d1e3-33a7-476b-d6eb-40ffc7783867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = 'google/bigbird-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHTRHs71PMn0",
        "outputId": "7d4e0368-5e94-43f0-ff0f-64357746a58c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90th percentile: 196.0\n",
            "91th percentile: 199.0\n",
            "92th percentile: 202.0\n",
            "93th percentile: 207.0\n",
            "94th percentile: 212.0\n",
            "95th percentile: 218.0\n",
            "96th percentile: 228.0\n",
            "97th percentile: 240.0\n",
            "98th percentile: 263.0\n",
            "99th percentile: 303.0\n",
            "100th percentile: 566.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate percentiles\n",
        "percentiles = np.percentile([len((i).split()) for i in race_train_high['cleaned_article']] , range(90, 101))\n",
        "\n",
        "# Print the percentile\n",
        "for p, percentile_value in zip(range(90, 101), percentiles):\n",
        "    print(f\"{p}th percentile: {percentile_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2CsbPYDcTxO",
        "outputId": "30f26078-b0aa-46ec-9baa-3dd26b636d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90th percentile: 15.0\n",
            "91th percentile: 15.0\n",
            "92th percentile: 15.0\n",
            "93th percentile: 16.0\n",
            "94th percentile: 16.0\n",
            "95th percentile: 16.0\n",
            "96th percentile: 17.0\n",
            "97th percentile: 18.0\n",
            "98th percentile: 19.0\n",
            "99th percentile: 20.0\n",
            "100th percentile: 63.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate percentiles\n",
        "percentiles = np.percentile([len((i).split()) for i in race_train_high['questions']] , range(90, 101))\n",
        "\n",
        "# Print the percentile\n",
        "for p, percentile_value in zip(range(90, 101), percentiles):\n",
        "    print(f\"{p}th percentile: {percentile_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0R9UgNocY5s",
        "outputId": "044d1fdb-80aa-4783-e47b-095788a29895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90th percentile: 10.0\n",
            "91th percentile: 10.0\n",
            "92th percentile: 11.0\n",
            "93th percentile: 11.0\n",
            "94th percentile: 11.0\n",
            "95th percentile: 12.0\n",
            "96th percentile: 12.0\n",
            "97th percentile: 13.0\n",
            "98th percentile: 13.0\n",
            "99th percentile: 15.0\n",
            "100th percentile: 91.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate percentiles\n",
        "percentiles = np.percentile([len((i[0]).split()) for i in race_train_high['options']] , range(90, 101))\n",
        "\n",
        "# Print the percentile\n",
        "for p, percentile_value in zip(range(90, 101), percentiles):\n",
        "    print(f\"{p}th percentile: {percentile_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeimJB2gBnkP"
      },
      "source": [
        "we did percentile check to find out length of the most the text. Length of most of the context lies below 300 and length of most of the question and option lies in between 15-20.\n",
        "here we are taking 350 max length of the text by combining all three of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsAk08O6BeoU"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def tokenizing_df(dataset):\n",
        "\n",
        "    max_seq_length = 350\n",
        "    input_ids= np.zeros((len(dataset), 4, max_seq_length), dtype=\"int32\")\n",
        "    token_type_ids= np.zeros((len(dataset), 4, max_seq_length), dtype=\"int32\")\n",
        "    attention_mask= np.zeros((len(dataset), 4, max_seq_length), dtype=\"int32\")\n",
        "    labels= []\n",
        "\n",
        "    ans={'A': 0,\n",
        "        'B': 1,\n",
        "        'C': 2,\n",
        "        'D': 3}\n",
        "    # input_ids, token_type_ids, attention_mask, labels= [], [], [], []\n",
        "    for i, row in tqdm(dataset.iterrows()):\n",
        "\n",
        "        # adding [CLS] at the begining and [SEP] at the end of the sentence\n",
        "        # input_ids_1, token_type_ids_1, attention_mask_1= [], [], []\n",
        "\n",
        "        for j, option in enumerate(row['options']):\n",
        "\n",
        "            pair = row['questions'] + '[SEP]' +  option + '[SEP]' + row[\"cleaned_article\"]\n",
        "\n",
        "            tokens = tokenizer.encode(pair)\n",
        "\n",
        "            input_ids[i, j, :]= np.array(tokens + [0]*(max_seq_length-len(tokens)))[:350]\n",
        "\n",
        "            # token_type_ids[i, j, :]= np.array([0]* max_seq_length)[:350]\n",
        "\n",
        "            attention_mask[i, j, :]= np.array([1]* len(tokens) + [0]* (max_seq_length-len(tokens)))[:350]\n",
        "\n",
        "        # input_ids.append(input_ids_1)\n",
        "        # token_type_ids.append(token_type_ids_1)\n",
        "        # attention_mask.append(attention_mask_1)\n",
        "\n",
        "\n",
        "        labels.append(ans[row['answers']])\n",
        "\n",
        "\n",
        "    df ={\n",
        "        \"input_ids\": input_ids,\n",
        "        # \"token_type_ids\": token_type_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noFU83YFJFBh"
      },
      "outputs": [],
      "source": [
        "# function to tokenize data\n",
        "\n",
        "def tokenizing_df(data):\n",
        "\n",
        "    # we are taking 350 sequence 95 percentile of context length\n",
        "    max_seq_length = 350\n",
        "\n",
        "    # labels = np.zeros((len(data), 4), dtype=\"int32\")\n",
        "\n",
        "    ans={'A': 0,\n",
        "         'B': 1,\n",
        "         'C': 2,\n",
        "         'D': 3}\n",
        "\n",
        "    # adding [CLS] at the begining and [SEP] at the end of the sentence\n",
        "    #pair =  [data['questions'] + [' <sep> '] + data[\"cleaned_article\"]] * 4\n",
        "    first_sentences = [[data['questions'][i] + '<s>' + data[\"cleaned_article\"][i]] * 4 for i in range(len(data['questions']))]\n",
        "\n",
        "    first_sentences = sum(first_sentences, [])\n",
        "    options= sum(data['options'], [])\n",
        "\n",
        "\n",
        "    # converting tokens to unique IDs\n",
        "    tokens = tokenizer(first_sentences, options, max_length= max_seq_length, padding= 'max_length',\n",
        "                        truncation= True, return_tensors=\"np\")\n",
        "\n",
        "    # return tokens, np.array(labels)\n",
        "    df= {\n",
        "        k: [v[i : i + 4] for i in range(0, len(v), 4)]\n",
        "        for k, v in tokens.items()\n",
        "    }\n",
        "\n",
        "    # labels = []\n",
        "    df['labels'] =  tf.convert_to_tensor([ans[i] for i in data['answers']], dtype=tf.int32)\n",
        "    # df['labels'] =  tf.convert_to_tensor([lst[ans[i]] for idx, labels in data['answers']], dtype=tf.int32)\n",
        "    # for idx, label in enumerate(data['answers']):\n",
        "    #   lst = np.zeros(4, dtype ='int32')\n",
        "    #   lst[ans[label]] = 1\n",
        "    #   labels.append(lst)\n",
        "    # df['labels'] = labels\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCWKmGLH-iaa"
      },
      "outputs": [],
      "source": [
        "# concatinating high and medium dataset from all category\n",
        "\n",
        "race_train = Dataset.from_pandas(pd.concat([race_train_high, race_train_medium], axis=0).reset_index(drop=True))\n",
        "race_dev = Dataset.from_pandas(pd.concat([race_dev_high, race_dev_medium], axis=0).reset_index(drop=True))\n",
        "race_test = Dataset.from_pandas(pd.concat([race_test_high, race_test_medium], axis=0).reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "d7ddda7fcc954d1c9c3ae9c71d6af3a1",
            "c84a18bc9dcc40f1838675a128f41c41",
            "2e9fed85d5b4444aa376554a22901b51",
            "814eda4157854d3f83e922ef70b172cb",
            "6dd4f8aed2814c8a91b8ae42129941fa",
            "9a9693c5caa843d4aa24e4ead1cab557",
            "f907abbdc05c4cb7b7f103fd8123a485",
            "fac70b4941b74e8cbd0cef84cf4d5e40",
            "721bae7a8de845288a7e989c32f0c457",
            "df19b0b11b2243809163f2b78e213be8",
            "9ef0e8ca604f4ccebb9c814a0f871a8a",
            "14f7ca4a44af4ebfb743b7f902cfde27",
            "db452196e7d645eeb227541fcda1bb86",
            "c1056595414e43c9831226f7ce2e7209",
            "0a4dcf686dc447378c8f929490540afb",
            "4a91960bc2bb47ab90d735b938c33bf7",
            "48068f45404449f88cd3e03ee90e0b71",
            "c1e19ef04d8f4ffd82ec175df06f6a5b",
            "7c722bcb9ca94522ab3c85d5a5bbb372",
            "b4a05f3015cb48bb8d03944b6ec6ca56",
            "81ae7db985824d5ba306117054e33f35",
            "4f483c23920a404e9ace2fed01f21093",
            "6cb1b04823584eb78d23f8b72c25bbda",
            "70ad159ea202440f97ef8e26a62913a0",
            "fadf02c2f0334e25b248aeeb6b727d30",
            "358a85c66bdb4db8ab63dfba79e194c9",
            "da9a61818159470bb56c6dcca4fe0d8a",
            "0417880308d74b92a424a3eed0c79141",
            "fd7e5ee7fbbf4a638d3ff30d908c7080",
            "33408ac11c2e4cff83ef31a82862629f",
            "23052ad056414d9bbe5758d8664a1a2d",
            "e9467559d8d74ba5a7301b00a66a18f0",
            "1c17ed6a258848d4b71cecce5d8dd491"
          ]
        },
        "id": "QiacTmRd_nHC",
        "outputId": "d2d785ec-7eae-4983-83a2-7839cd835971"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/87866 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7ddda7fcc954d1c9c3ae9c71d6af3a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4887 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14f7ca4a44af4ebfb743b7f902cfde27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4934 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cb1b04823584eb78d23f8b72c25bbda"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# tokenizing\n",
        "race_train =  race_train.map(tokenizing_df, batched =True,  batch_size= 10, remove_columns= ['answers',\t'options',\t'questions',\t'article',\t'id',\t'cleaned_article'])\n",
        "race_dev = race_dev.map(tokenizing_df, batched =True, batch_size= 10,  remove_columns= ['answers',\t'options',\t'questions',\t'article',\t'id',\t'cleaned_article'])\n",
        "race_test = race_test.map(tokenizing_df, batched =True,  batch_size= 10, remove_columns= ['answers',\t'options',\t'questions',\t'article',\t'id',\t'cleaned_article'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0l3DxsmBaDa"
      },
      "source": [
        "#### loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR6-GHrpVeD0"
      },
      "outputs": [],
      "source": [
        "pickle.dump((race_train, race_dev, race_test),\n",
        "            open('/content/drive/MyDrive/race_tokenized_bigbird_350.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-H7ntJK2_N"
      },
      "outputs": [],
      "source": [
        "race_train, race_dev, race_test= \\\n",
        "pickle.load(open('/content/drive/MyDrive/race_tokenized_bigbird_350.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfttsFaYByrl"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NS65kMMByTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1a74a2-0370-49ea-dfac-8c78897541b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cpu)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.6 torchmetrics-1.4.1\n"
          ]
        }
      ],
      "source": [
        "# os.environ['XLA_USE_BF32'] = \"1\"\n",
        "# os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
        "\n",
        "import torch\n",
        "# import torch_xla\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from  accelerate import  Accelerator, DistributedType\n",
        "from accelerate import notebook_launcher\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import datasets\n",
        "import transformers\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.distributed.parallel_loader as pl\n",
        "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "# import torch_xla.debug.metrics as met\n",
        "# import torch_xla.utils.utils as xu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp4h-0hdETZO"
      },
      "outputs": [],
      "source": [
        "# converting data to pytorch tensor\n",
        "race_train.set_format(\"torch\")\n",
        "race_dev.set_format(\"torch\")\n",
        "race_test.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VLCkCPoEq1-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "e014b9d30cbc447bb1c19105d5275711",
            "00fccdd3c45f4c52bcfd480bd0306315",
            "a3172acd4c8842dab0b8972c1bec6029",
            "42f2ab0ef7254f6487102f6808eeca8e",
            "6bf72940140e4eaaabb45692cc2bff1e",
            "198d37c23d564ea09d1ceb6338a5676f",
            "bc11af7b34e34212a2110079d22e1136",
            "10e00e9084a845b3931c19f437ac763e",
            "6023f3bea1cc459d8ee09d5caa68f41b",
            "695b30adb17a4a589a8ad5aa357d6c96",
            "4e0e36cbfcf4438b9b538f4dd4eea421"
          ]
        },
        "outputId": "d96178c5-1188-43d4-9a40-55fe94e21230"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e014b9d30cbc447bb1c19105d5275711"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForMultipleChoice were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BigBirdConfig, AutoModelForMultipleChoice\n",
        "\n",
        "config = BigBirdConfig.from_pretrained(model_name)\n",
        "config.attention_type = \"original_full\"\n",
        "\n",
        "model = AutoModelForMultipleChoice.from_pretrained(model_name, config=config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O11TyqjoFAYO"
      },
      "outputs": [],
      "source": [
        "# setting hyperparameters as mentioned by the author\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"num_epochs\": 1, # due to time restriction for the TPU usage in google colab we are training for 1 epoch\n",
        "    \"train_batch_size\": 32, # taking 1 batch size due to fit in a memory\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"max_grad\": 1.0,\n",
        "    \"adam_epsilon\": 1e-6,\n",
        "    \"num_warmup_steps\": 1000,\n",
        "    \"seed\": 42,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7c_Wf7QE8Bp"
      },
      "outputs": [],
      "source": [
        "# creating dataloader to prepare data set for model training\n",
        "def create_dataloaders(train_batch_size, eval_batch_size):\n",
        "    train_dataloader = DataLoader(\n",
        "        race_train, shuffle=True, batch_size=train_batch_size\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        race_dev, shuffle=False, batch_size=eval_batch_size\n",
        "    )\n",
        "    return train_dataloader, eval_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l8hOFvlbmz8"
      },
      "outputs": [],
      "source": [
        "train_dataloader, eval_dataloader = create_dataloaders(\n",
        "    train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDBqaTG3PC6s"
      },
      "source": [
        "#### experiment with large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAEidlzcPC7L"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5WDCpppPC7M"
      },
      "outputs": [],
      "source": [
        "def training_function(model):\n",
        "    # Initialize accelerator\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
        "    # to INFO for the main process only.\n",
        "    if accelerator.is_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    train_dataloader, eval_dataloader = create_dataloaders(\n",
        "        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n",
        "    )\n",
        "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
        "    set_seed(hyperparameters[\"seed\"])\n",
        "\n",
        "    # Instantiate optimizer with learning rate and adam epsilon\n",
        "    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"],\n",
        "                      eps= hyperparameters['adam_epsilon'], no_deprecation_warning=True)\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
        "    # prepare method.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    num_epochs = hyperparameters[\"num_epochs\"]\n",
        "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
        "    # may change its length.\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=hyperparameters['num_warmup_steps'],\n",
        "        num_training_steps=len(train_dataloader) * num_epochs,\n",
        "    )\n",
        "    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
        "    # process to avoid having 8 progress bars.\n",
        "    accelerator.print('Training started...')\n",
        "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
        "\n",
        "    # Now we train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), hyperparameters[\"max_grad\"])\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "            if step % 100 == 0 or step == len(train_dataloader)-1:\n",
        "              accelerator.print(f'Epoch: {epoch+1}, Step: {step}, Loss: {loss.item()}')\n",
        "\n",
        "            # if step == 30:\n",
        "            #   break\n",
        "        # initializing model for evaluation\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "            # We gather predictions and labels from the 8 TPUs to have them all.\n",
        "            all_predictions.append(accelerator.gather(predictions))\n",
        "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
        "\n",
        "        # Concatenate all predictions and labels.\n",
        "        # The last thing we need to do is to truncate the predictions and labels we concatenated\n",
        "        # together as the prepared evaluation dataloader has a little bit more elements to make\n",
        "        # batches of the same size on each process.\n",
        "        all_predictions = torch.cat(all_predictions)[:len(race_dev)]\n",
        "        all_labels = torch.cat(all_labels)[:len(race_dev)]\n",
        "\n",
        "        # first we have to get all the metrices to cpu and then we will convert to numpy array\n",
        "        # now we will accuracy score out of both\n",
        "        eval_metric = accuracy_score(all_labels.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "\n",
        "        # # Use accelerator.print to print only on the main process.\n",
        "        accelerator.print(f\"Epoch: {epoch+1}, Accuracy_score= {eval_metric}, Loss= {loss.item()}\")\n",
        "\n",
        "    # at the end we will unwrap model from all the TPU cores\n",
        "    # and then will save model's trained weights\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "\n",
        "    xser.save(unwrapped_model.state_dict(), f\"/content/drive/MyDrive/model.bin\", master_only=True)\n",
        "    # accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/ARD on Race pretrained model/model.pt')\n",
        "    # torch.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/ARD on Race pretrained model/')\n",
        "    # unwrapped_model.save_pretrained('/content/drive/MyDrive/ARD on Race pretrained model/', save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYvaINDBPC7M"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "notebook_launcher(training_function, (model,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smXQ3yZ1SYvr",
        "outputId": "eed54373-606e-4369-eb2a-516672b056f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available distributed types: DistributedType.XLA\n"
          ]
        }
      ],
      "source": [
        "print(\"Available distributed types:\", DistributedType.XLA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8jt2XZvpqvH"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(xser.load(f\"model.bin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFBRSwiVygGN"
      },
      "source": [
        "####poo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEcfcG8j5XxX"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KbGTQtRFcOP"
      },
      "outputs": [],
      "source": [
        "def training_function(model):\n",
        "    # Initialize accelerator\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
        "    # to INFO for the main process only.\n",
        "    if accelerator.is_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    train_dataloader, eval_dataloader = create_dataloaders(\n",
        "        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n",
        "    )\n",
        "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
        "    set_seed(hyperparameters[\"seed\"])\n",
        "\n",
        "    # Instantiate optimizer with learning rate and adam epsilon\n",
        "    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"],\n",
        "                      eps= hyperparameters['adam_epsilon'], no_deprecation_warning=True)\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
        "    # prepare method.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    num_epochs = hyperparameters[\"num_epochs\"]\n",
        "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
        "    # may change its length.\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=hyperparameters['num_warmup_steps'],\n",
        "        num_training_steps=len(train_dataloader) * num_epochs,\n",
        "    )\n",
        "    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
        "    # process to avoid having 8 progress bars.\n",
        "    accelerator.print('Training started...')\n",
        "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
        "\n",
        "    # Now we train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), hyperparameters[\"max_grad\"])\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "            # print loss after each 1000 steps\n",
        "            if step % 1000 == 0 or step == len(train_dataloader)-1:\n",
        "              accelerator.print(f'Epoch: {epoch+1}, Step: {step}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "        # initializing model for evaluation\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "            # We gather predictions and labels from the 8 TPUs to have them all.\n",
        "            all_predictions.append(accelerator.gather(predictions))\n",
        "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
        "\n",
        "        # Concatenate all predictions and labels.\n",
        "        # The last thing we need to do is to truncate the predictions and labels we concatenated\n",
        "        # together as the prepared evaluation dataloader has a little bit more elements to make\n",
        "        # batches of the same size on each process.\n",
        "        all_predictions = torch.cat(all_predictions)[:len(race_dev)]\n",
        "        all_labels = torch.cat(all_labels)[:len(race_dev)]\n",
        "\n",
        "        # first we have to get all the metrices to cpu and then we will convert to numpy array\n",
        "        # now we will accuracy score out of both\n",
        "        eval_metric = accuracy_score(all_labels.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "\n",
        "        # # Use accelerator.print to print only on the main process.\n",
        "        accelerator.print(f\"Epoch: {epoch+1}, Accuracy_score= {eval_metric}, Loss= {loss.item()}\")\n",
        "\n",
        "    # at the end we will unwrap model from all the TPU cores\n",
        "    # and then will save model's trained weights\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/ARD on Race pretrained model/model.pt')\n",
        "    # torch.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/ARD on Race pretrained model/')\n",
        "    # unwrapped_model.save_pretrained('/content/drive/MyDrive/ARD on Race pretrained model/', save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "2c8b7fdb11564019b3532bf1ae8fec9c"
          ]
        },
        "id": "SelQWlYRH1GD",
        "outputId": "cbac2fbc-53f7-4239-aed9-0e2c7d78ac53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching a training on 8 TPU cores.\n",
            "Training started...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c8b7fdb11564019b3532bf1ae8fec9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10984 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Step: 0, Loss: 1.4211560487747192\n",
            "Epoch: 1, Step: 1000, Loss: 1.5400097370147705\n",
            "Epoch: 1, Step: 2000, Loss: 1.1186223030090332\n",
            "Epoch: 1, Step: 3000, Loss: 1.1558825969696045\n",
            "Epoch: 1, Step: 4000, Loss: 0.5302114486694336\n",
            "Epoch: 1, Step: 5000, Loss: 7.061692714691162\n",
            "Epoch: 1, Step: 6000, Loss: 0.3434958755970001\n",
            "Epoch: 1, Step: 7000, Loss: 0.0021830059122294188\n",
            "Epoch: 1, Step: 8000, Loss: 0.002069066045805812\n",
            "Epoch: 1, Step: 9000, Loss: 1.275923490524292\n",
            "Epoch: 1, Step: 10000, Loss: 1.740681767463684\n",
            "Epoch: 1, Step: 10983, Loss: 0.26758986711502075\n",
            "Epoch: 1, Accuracy_score= 0.5281358706773072, Loss= 0.27765798568725586\n",
            "CPU times: user 1min 57s, sys: 15.8 s, total: 2min 13s\n",
            "Wall time: 5h 20min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "notebook_launcher(training_function, (model,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0h_eIog1Kud"
      },
      "source": [
        "## inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWRmW62DqCVb",
        "outputId": "6fdbb29f-0890-414c-86bf-26759f52633d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/AVD on Race pretrained model/model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjA_CX4dMcvg",
        "outputId": "25c0a36f-7056-4d87-f10e-8fef060d1a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yjnzeFxx83Z0AWGjk8B4rPp2Sl12bUFX\n",
            "To: /content/arc_datasets_scores_easy.pkl\n",
            "100% 96.9M/96.9M [00:00<00:00, 117MB/s] \n"
          ]
        }
      ],
      "source": [
        "# downloading Arc Dataset with BM25 and DRD scores\n",
        "!gdown 1yjnzeFxx83Z0AWGjk8B4rPp2Sl12bUFX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afro3WdIb00s"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "train_challenge, dev_challenge, test_challenge=\n",
        "pickle.load(open('/content/arc_datasets_scores_challenge.pkl', 'rb'))\n",
        "\n",
        "train_easy, dev_easy, test_easy= \\\n",
        "pickle.load(open('/content/arc_datasets_scores_easy.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57vC9gKah9Ek"
      },
      "outputs": [],
      "source": [
        "def joining_scores(data, new_scores):\n",
        "  '''\n",
        "  function to join existing DRD scores with AVD scores\n",
        "  '''\n",
        "    old_scores = data['score']\n",
        "    scores =[]\n",
        "    for idx_1, score_1 in enumerate(old_scores):\n",
        "        list_of_scores= []\n",
        "        for idx_2, score_2 in enumerate(score_1):\n",
        "\n",
        "            list_of_scores.append([score_3 + [new_scores[idx_1][idx_2][idx_3]] for idx_3, score_3 in enumerate(score_2)])\n",
        "        scores.append(list_of_scores)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating AVD scores"
      ],
      "metadata": {
        "id": "piQePLI3ePRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f-5T9OgLxtl"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def avd_score(model, tokenizer, data):\n",
        "  '''\n",
        "  function to get AVD scores from downstream model trained on RACE data set\n",
        "  '''\n",
        "    # device = xm.xla_device()\n",
        "    device = torch.device('cuda')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    question= data['only_questions']\n",
        "    answer= data['only_answers']\n",
        "    context = data['context']\n",
        "    scores = []\n",
        "\n",
        "    # it will iterate through each question\n",
        "    for i, que in enumerate(tqdm(question)):\n",
        "        list_of_scores= []\n",
        "        # it will iterate through each answer of that particular question\n",
        "        # each question has four option so it will iterate four times\n",
        "        for j, ans in enumerate(answer[i]):\n",
        "            # it will get contexts of particular question with particular option\n",
        "            # it will get 50 contexts\n",
        "            context_len = len(context[i][j])\n",
        "\n",
        "            # we will pair question and answer and multiply with 50 times to match context length\n",
        "            pair= [que + '</s>' + ans] * context_len\n",
        "\n",
        "            # now we have 50 pairs of ques and ans and 50 contexts\n",
        "            # now we will tokenize them\n",
        "            tokens= tokenizer(pair, context[i][j], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            input_ids= torch.unsqueeze(tokens['input_ids'], dim=0).to(device)\n",
        "            attention_mask= torch.unsqueeze(tokens['attention_mask'], dim=0).to(device)\n",
        "\n",
        "            # this below code will get softmax scores of each question, answer and context pair which is 50\n",
        "            with torch.no_grad():\n",
        "\n",
        "              logits= model(input_ids, attention_mask).logits\n",
        "            logits.detach()\n",
        "            softmax = torch.nn.functional.softmax(logits[0], dim= -1)\n",
        "            list_of_scores.append(softmax.tolist())\n",
        "\n",
        "        # now we will append softmax scores to the list of all 50 scores\n",
        "        # each question have 4 options and each option has 50 context scores\n",
        "        scores.append(list_of_scores)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8552cd-1e68-42ca-9552-746cecac7b4b",
        "id": "KvUFNbLAh9Et"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1119/1119 [29:07<00:00,  1.56s/it]\n",
            "100%|██████████| 299/299 [05:07<00:00,  1.03s/it]\n",
            "100%|██████████| 1172/1172 [20:53<00:00,  1.07s/it]\n"
          ]
        }
      ],
      "source": [
        "train_challenge['score']= joining_scores(train_challenge, avd_score(model,tokenizer, train_challenge))\n",
        "dev_challenge['score']= joining_scores(dev_challenge, avd_score(model,tokenizer, dev_challenge))\n",
        "test_challenge['score']= joining_scores(test_challenge, avd_score(model,tokenizer, test_challenge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG3kOpm6dYMF"
      },
      "outputs": [],
      "source": [
        "pickle.dump((train_challenge, dev_challenge, test_challenge),\n",
        "            open('/content/drive/MyDrive/arc_dataset_challnege_final_scores.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPpA9u4bdudL",
        "outputId": "ba19ef73-4b96-4746-a754-3e1e9c5d12bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2251/2251 [50:32<00:00,  1.35s/it]\n",
            "100%|██████████| 570/570 [10:10<00:00,  1.07s/it]\n",
            "100%|██████████| 2376/2376 [37:02<00:00,  1.07it/s]\n"
          ]
        }
      ],
      "source": [
        "train_easy['score']= joining_scores(train_easy, avd_score(model,tokenizer, train_easy))\n",
        "dev_easy['score']= joining_scores(dev_easy, avd_score(model, tokenizer,dev_easy))\n",
        "test_easy['score']= joining_scores(test_easy, avd_score(model,tokenizer, test_easy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGRihOmddw-z"
      },
      "outputs": [],
      "source": [
        "pickle.dump((train_easy, dev_easy, test_easy),\n",
        "            open('/content/drive/MyDrive/arc_dataset_easy_final_scores.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja90Z6hFBucZ"
      },
      "outputs": [],
      "source": [
        "train_challenge, dev_challenge, test_challenge= \\\n",
        "pickle.load(open('/content/drive/MyDrive/arc_dataset_challnege_final_scores.pkl', 'rb'))\n",
        "\n",
        "train_easy_old, dev_easy_old, test_easy_old= \\\n",
        "pickle.load(open('/content/drive/MyDrive/arc_dataset_easy_final_scores.pkl', 'rb'))"
      ]
    }
  ]
}